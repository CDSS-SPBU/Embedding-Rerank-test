import asyncio
import logging
import time
from transformers import AutoTokenizer, AutoModel
import torch
import torch.nn.functional as F
import numpy as np
import sys

logging.basicConfig(
    level=getattr(logging, "INFO"),
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class JinaEmbedder:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å jina-embeddings-v3"""

    def __init__(self, model_name="jinaai/jina-embeddings-v3"):
        logger.info("üîç –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ %s", model_name)
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: %s", self.device)

        try:
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_name, trust_remote_code=True
            )
            self.model = AutoModel.from_pretrained(
                model_name, trust_remote_code=True
            ).to(self.device)
            self.model.eval()
        except Exception as e:
            logger.error("–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: %s", e)
            raise
        logger.info("–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

    def encode(
        self, texts: list, task="retrieval.passage", max_length=8192, dimensions=1024
    ):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤

        Args:
            texts (list): —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ —Ä–∞–∑–º–µ—Ä–∞ n
            task (str, optional): "retrieval.query" –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞. –¥–ª—è —á–∞–Ω–∫–∞ "retrieval.passage".
            max_length (int, optional): . Defaults to 8192.
            dimensions (int, optional): —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞.
                Defaults to 1024(–¥—Ä—É–≥–∏–µ –≤–∞–ª–∏–¥–Ω—ã–µ 768, 512, 384, 256, 128, 64).

        Returns:
            numpy.ndarray: —ç–º–±–µ–¥–¥–∏–Ω–≥ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ (1, dimensions) –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ –∏ (n, dimensions) –¥–ª—è —á–∞–Ω–∫–æ–≤
        """
        if not texts:
            raise ValueError("–°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –Ω–µ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø—É—Å—Ç—ã–º")

        logger.debug(
            "–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ %d —Ç–µ–∫—Å—Ç–æ–≤, –∑–∞–¥–∞—á–∞=%s, max_length=%d, dim=%d",
            len(texts),
            task,
            max_length,
            dimensions,
        )

        try:
            prefixed = [f"<{task}>{text}" for text in texts]
            batch = self.tokenizer(
                prefixed,
                padding=True,
                truncation=True,
                max_length=max_length,
                return_tensors="pt",
                add_special_tokens=True,
            ).to(self.device)

            with torch.no_grad():
                outputs = self.model(**batch)
                last_hidden = outputs.last_hidden_state

            # Mean pooling
            attention_mask = batch["attention_mask"]
            mask = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()
            sum_emb = torch.sum(last_hidden * mask, dim=1)
            sum_mask = torch.clamp(mask.sum(1), min=1e-9)
            mean_pooled = sum_emb / sum_mask

            # Matryoshka truncation
            if dimensions != 1024:
                if dimensions > 1024:
                    raise ValueError("–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –±–æ–ª—å—à–µ 1024")
                mean_pooled = mean_pooled[:, :dimensions]
                logger.debug("–ü—Ä–∏–º–µ–Ω–µ–Ω–æ —É—Å–µ—á–µ–Ω–∏–µ Matryoshka –¥–æ %dD", dimensions)

            # Normalize
            embeddings = F.normalize(mean_pooled, p=2, dim=1)
            result = embeddings.cpu().numpy().astype(np.float32)

            del batch, outputs, last_hidden
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            logger.debug(" –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ %d —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤", len(result))
            return result
        except Exception as e:
            logger.error("–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–æ–≤: %s", e)
            raise

    async def encode_async(self, texts: list, **kwargs):
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, lambda: self.encode(texts, **kwargs))

async def benchmark():
    embedder = JinaEmbedder()
    texts = ["test text"] * 10
    
    # –°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤
    start = time.time()
    for i in range(5):
        embedder.encode(texts)
    sync_time = time.time() - start
    
    # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤
    start = time.time()
    tasks = [embedder.encode_async(texts) for _ in range(5)]
    await asyncio.gather(*tasks)
    async_time = time.time() - start
    
    print(f"–°–∏–Ω—Ö—Ä–æ–Ω–Ω–æ: {sync_time:.2f}—Å")
    print(f"–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ: {async_time:.2f}—Å")
    print(f"–£—Å–∫–æ—Ä–µ–Ω–∏–µ: {sync_time/async_time:.2f}x")

if __name__ == "__main__":
    asyncio.run(benchmark())