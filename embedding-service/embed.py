import logging
from transformers import AutoTokenizer, AutoModel
import torch
import torch.nn.functional as F
import numpy as np
import os
import asyncio
from concurrent.futures import ThreadPoolExecutor

logging.basicConfig(
    level=getattr(logging, os.getenv("LOG_LEVEL", "INFO")),
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class JinaEmbedder:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å jina-embeddings-v3"""

    def __init__(self, model_name="jinaai/jina-embeddings-v3", max_workers=3):
        logger.info("üîç –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ %s", model_name)
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: %s", self.device)

        # –ü—É–ª –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ—Å—Ç–∏
        self.thread_pool = ThreadPoolExecutor(max_workers=max_workers)


        try:
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_name, trust_remote_code=True
            )
            self.model = AutoModel.from_pretrained(
                model_name, trust_remote_code=True
            ).to(self.device)
            self.model.eval()
        except Exception as e:
            logger.error("–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: %s", e)
            raise
        logger.info("–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

    def encode(
        self, texts: list, task="retrieval.passage", max_length=8192, dimensions=1024
    ) -> np.ndarray:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤

        Args:
            texts (list): —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ —Ä–∞–∑–º–µ—Ä–∞ n
            task (str, optional): "retrieval.query" –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞. –¥–ª—è —á–∞–Ω–∫–∞ "retrieval.passage".
            max_length (int, optional): . Defaults to 8192.
            dimensions (int, optional): —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞.
                Defaults to 1024(–¥—Ä—É–≥–∏–µ –≤–∞–ª–∏–¥–Ω—ã–µ 768, 512, 384, 256, 128, 64).

        Returns:
            numpy.ndarray: —ç–º–±–µ–¥–¥–∏–Ω–≥ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ (1, dimensions) –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ –∏ (n, dimensions) –¥–ª—è —á–∞–Ω–∫–æ–≤
        """
        if not texts:
            raise ValueError("–°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –Ω–µ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø—É—Å—Ç—ã–º")

        logger.debug(
            "–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ %d —Ç–µ–∫—Å—Ç–æ–≤, –∑–∞–¥–∞—á–∞=%s, max_length=%d, dim=%d",
            len(texts),
            task,
            max_length,
            dimensions,
        )

        try:
            prefixed = [f"<{task}>{text}" for text in texts]
            batch = self.tokenizer(
                prefixed,
                padding=True,
                truncation=True,
                max_length=max_length,
                return_tensors="pt",
                add_special_tokens=True,
            ).to(self.device)

            with torch.no_grad():
                outputs = self.model(**batch)
                last_hidden = outputs.last_hidden_state

            # Mean pooling
            attention_mask = batch["attention_mask"]
            mask = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()
            sum_emb = torch.sum(last_hidden * mask, dim=1)
            sum_mask = torch.clamp(mask.sum(1), min=1e-9)
            mean_pooled = sum_emb / sum_mask

            # Matryoshka truncation
            if dimensions != 1024:
                if dimensions > 1024:
                    raise ValueError("–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –±–æ–ª—å—à–µ 1024")
                mean_pooled = mean_pooled[:, :dimensions]
                logger.debug("–ü—Ä–∏–º–µ–Ω–µ–Ω–æ —É—Å–µ—á–µ–Ω–∏–µ Matryoshka –¥–æ %dD", dimensions)

            # Normalize
            embeddings = F.normalize(mean_pooled, p=2, dim=1)
            result = embeddings.cpu().numpy().astype(np.float32)

            del batch, outputs, last_hidden
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            logger.debug(" –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ %d —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤", len(result))
            return result
        except Exception as e:
            logger.error("–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–æ–≤: %s", e)
            raise

    async def encode_async(self, texts: list, task="retrieval.passage", max_length=8192, dimensions=1024):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è encode"""
        loop = asyncio.get_event_loop()
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –≤ thread pool —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å event loop
        return await loop.run_in_executor(
            self.thread_pool, 
            lambda: self.encode(texts, task, max_length, dimensions)
        )
